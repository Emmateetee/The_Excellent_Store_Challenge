{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test= pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work about to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine():\n",
    "    def __init__(self, test, train):\n",
    "        self.test = test\n",
    "        self.train = train\n",
    "    def getTarget(self):\n",
    "        return self.train['Item_Store_Returns']\n",
    "        \n",
    "    def join(self):\n",
    "        copyTrain = self.train.copy()\n",
    "        copyTest = self.test.copy()\n",
    "        \n",
    "        copyTrain.drop('Item_Store_Returns', 1, inplace=True)\n",
    "        self.data = copyTrain.append(copyTest)\n",
    "        return copyTrain.append(copyTest)\n",
    "    \n",
    "    def get_shape(self):\n",
    "        trainRows = self.train.shape[0]\n",
    "        testRows = self.test.shape[0]\n",
    "        df_shape = dict()\n",
    "        df_shape['train'] = trainRows\n",
    "        df_shape['test'] = testRows\n",
    "        return df_shape\n",
    "    \n",
    "    def heatmap(self, df, method):\n",
    "        self.df = df\n",
    "        self.method = method\n",
    "        if self.method == 'isnull':\n",
    "            return sns.heatmap(self.df.isnull())\n",
    "        elif self.method == 'corr':\n",
    "            return sns.heatmap(self.df.corr(), annot=True)\n",
    "    \n",
    "    def get_null(self):\n",
    "        return self.data.isna().sum()\n",
    "    \n",
    "    def drop_null(self):\n",
    "        for x in self.data:\n",
    "            count_null = self.data[x].isnull().sum();\n",
    "            percentage_null = (count_null/len(self.data) )* 100\n",
    "            if percentage_null > 60:\n",
    "                self.data.drop([x], 1, inplace=True)\n",
    "            elif self.data[x].isnull().sum() > 0:\n",
    "                if self.data[x].dtypes == 'O':\n",
    "                      self.data[x] = self.data[x].fillna(self.data[x].mode()[0])\n",
    "                elif self.data[x].dtypes != 'O':\n",
    "                    self.data[x] = self.data[x].fillna(self.data[x].mean())\n",
    "        return self.data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_engine = Engine(test, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_engine.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_engine.getTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_engine.get_shape()['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_engine.heatmap(df, 'isnull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_engine.heatmap(df, 'corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_engine.get_null()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_engine.drop_null()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i will frequeny encode 'Item_ID', 'Store_ID', \n",
    "#### i will drop 'Item_Store_ID'\n",
    "#### i will freq encode 'Item_Weight' and multiply the value by the actual weignt in another feature\n",
    "#### i will onehot encode 'Item_Sugar_Content'\n",
    "#### i will cat encode 'Item_Type' and multiply it by the freq encode value\n",
    "#### i will freq encode Store_Start_Year\n",
    "#### i will cat encode Store_Size, \n",
    "#### i will cat encode Store_Location_Type and freq encode it, multiply cat encode and freq encode\n",
    "#### i will onehot encode Store_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngine():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def cat_encode(self, leF):\n",
    "        for x in leF:\n",
    "            self.df[x] = le.fit_transform(self.df[x])\n",
    "        return self.df\n",
    "    def oneHot(self, onhF):\n",
    "        self.df = pd.get_dummies(self.df, prefix_sep='_', columns=onhF)\n",
    "        return self.df\n",
    "    def freq_encode(self, freqF):\n",
    "        for x in freqF:\n",
    "            self.df[x] = self.df[x].map(self.df[x].value_counts())\n",
    "        return self.df\n",
    "    def shift_cat_from_zero(self, fets, data):\n",
    "        self.data = data\n",
    "        for x in fets:\n",
    "            self.data[x] = self.data[x].values + 1\n",
    "        return self.data\n",
    "    def drop_strong_corr(self, df_to_drop):\n",
    "        self.df_to_drop = df_to_drop\n",
    "        \n",
    "        corr_matrix = self.df_to_drop.corr().abs()\n",
    "        \n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "        \n",
    "        to_drop = [column for column in upper.columns if any(upper[column]) > 0.75]\n",
    "        \n",
    "        return self.df_to_drop.drop(to_drop, 1, inplace=True)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propel_feature = FeatureEngine(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_cat_encode = propel_feature.cat_encode(['Store_Location_Type', 'Store_Size', 'Item_Type' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_shifting = propel_feature.shift_cat_from_zero(['Store_Location_Type', 'Store_Size', 'Item_Type' ],after_cat_encode )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_onehot_encode = propel_feature.oneHot(['Store_Type', 'Item_Sugar_Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_onehot_encode['Store_Location_Type_freq'] = after_onehot_encode['Store_Location_Type'] \n",
    "after_onehot_encode['Item_Type_freq'] = after_onehot_encode['Item_Type'] \n",
    "after_onehot_encode['Item_Type_freq'] = after_onehot_encode['Item_Type'].values\n",
    "after_onehot_encode['Item_Weight_freq'] = after_onehot_encode['Item_Weight'].values\n",
    "\n",
    "after_freq_encode = propel_feature.freq_encode(['Store_Location_Type_freq', 'Item_Type_freq',\n",
    "                                                'Item_Weight_freq', 'Item_ID', 'Store_ID', \n",
    "                                                'Store_Start_Year', 'Item_Type_freq' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = after_freq_encode\n",
    "df.drop('Item_Store_ID', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Feature Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " standardise [ item visibility,  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Item_Visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MinMaxScaler()\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Item_Visibility'] = np.sqrt(df['Item_Visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['price_weignt'] = np.sqrt(df['Item_Price'] * df['Item_Weight'])\n",
    "df['visibility_store_size'] = (df['Item_Visibility'] * 10 )/df['Store_Size']\n",
    "df['item_weight_weight'] = df['Item_Weight_freq'] / df['Item_Weight']\n",
    "df['Item_Typefreq_weight'] = np.sqrt(df['Item_Type_freq'] * df['Item_Type'])\n",
    "df['Store_Location_Type_freq'] = np.sqrt(df['Store_Location_Type_freq'])\n",
    "df['Item_Typefreq_weight'] = ms.fit_transform(df['Item_Typefreq_weight'].values.reshape(-1,1))\n",
    "df['item_freq_price_weight'] = df['Item_Type_freq']/df['price_weignt']\n",
    "df['item_type_price_div'] = (df['Item_Price'] / df['Item_Type']) * df['Store_Size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Store_Location_Type_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "run_engine.heatmap(df, 'corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.65)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[to_drop],1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "run_engine.heatmap(df, 'corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbR = xgb.XGBRegressor(\n",
    "                        subsample=0.6, reg_lambda=20, \n",
    "                        random_state=40, gamma=0.1,\n",
    "                        n_estimators=6000,max_depth=7, \n",
    "                        learning_rate=0.06999, \n",
    "                        min_child_weight=3,\n",
    "                        colsample_bytree=0.1\n",
    "                        )\n",
    "\n",
    "carR = cat.CatBoostRegressor(subsample=0.3, reg_lambda=240, random_state=970, n_estimators=6000,max_depth=1, learning_rate=0.1)\n",
    "gbm = GradientBoostingRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "lgb = lgbm.LGBMRegressor(\n",
    "                    num_leaves= 20,\n",
    "                    min_data_in_leaf = 7,\n",
    "                    objective='regression',\n",
    "                    max_depth=9,learning_rate=0.12, \n",
    "                    boosting_type='gbdt', \n",
    "                    lambda_l2=300, \n",
    "                    metric='rmse', \n",
    "                    n_estimators=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ms.fit_transform(df)\n",
    "df = sc.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[: run_engine.get_shape()['train'] ]\n",
    "test = df[run_engine.get_shape()['train'] :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train\n",
    "y = run_engine.getTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=10\n",
    "folds = KFold(n_splits=kf, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_score(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train.ravel())\n",
    "    pred_y = model.predict(x_test)\n",
    "    return np.sqrt(mean_squared_error(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SebNn1():\n",
    "    def __init__(self, unitList, dimension):\n",
    "        self.unitList = unitList;\n",
    "        self.dimension = dimension;\n",
    "    def neuralize(self):\n",
    "    \n",
    "        algol = models.Sequential()\n",
    "\n",
    "        algol.add(layers.Dense(units=self.unitList[0], input_dim= self.dimension,  activation='relu'))\n",
    "\n",
    "        algol.add(layers.Dense(units=self.unitList[1],  activation='relu'))\n",
    "\n",
    "        algol.add(layers.Dense(units=self.unitList[2],  activation='relu'))\n",
    "\n",
    "        algol.add(layers.Dense(units=self.unitList[3],  activation='relu'))\n",
    "\n",
    "        algol.add(layers.Dense(units=1))\n",
    "\n",
    "        algol.compile(loss='mean_squared_error', optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "        return algol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seb_nn_1 = SebNn1([15,13,9,6], train.shape[1]).neuralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seb_nn_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagicBrain():\n",
    "    def __init__(self, x, y, test):\n",
    "        self.x = x \n",
    "        self.y = y\n",
    "        self.test = test\n",
    "        \n",
    "    def activate_cell(self,model,pt, learn_type, ep=0, bs=0,rounds=200):\n",
    "        score_list = list()\n",
    "        brain_info = dict()\n",
    "        \n",
    "        predicted_values = np.zeros(len(test))\n",
    "        \n",
    "\n",
    "        callback = callbacks.EarlyStopping(monitor='loss', patience=pt)\n",
    "        \n",
    "        for fold, (train_index, test_index) in enumerate(folds.split(self.x,self.y)):\n",
    "            print(f\"================================Fold{fold}====================================\")\n",
    "            x_train,x_test,y_train,y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "            \n",
    "            if learn_type == 'neurals':\n",
    "                model.fit(x_train, y_train, epochs=ep, verbose=1, callbacks=callback, batch_size=bs, validation_data=(x_test, y_test), shuffle=True)\n",
    "                score_list.append(np.sqrt( mean_squared_error( y_test, model.predict(x_test) ) ))\n",
    "                predicted_values += np.ravel(model.predict(self.test))\n",
    "                \n",
    "            elif learn_type == 'cat':\n",
    "                model.fit(x_train,y_train,eval_set=[(x_train,y_train),(x_test,y_test)],early_stopping_rounds=rounds,use_best_model=True)\n",
    "                score_list.append(np.sqrt( mean_squared_error( y_test, model.predict(x_test) ) ))\n",
    "                predicted_values += model.predict(self.test)\n",
    "        \n",
    "            else:\n",
    "                model.fit(x_train,y_train,eval_set=[(x_train,y_train),(x_test,y_test)],early_stopping_rounds=rounds)\n",
    "                score_list.append(np.sqrt( mean_squared_error( y_test, model.predict(x_test) ) ))\n",
    "                predicted_values += model.predict(self.test)\n",
    "                brain_info = dict()\n",
    "                \n",
    "                \n",
    "        mean_score = np.mean(score_list)\n",
    "        brain_info['RMSE score'] = mean_score\n",
    "        brain_info['predictions'] = predicted_values/kf\n",
    "        \n",
    "        return brain_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activate_brain = MagicBrain(x,y,test).activate_cell(seb_nn_1, ep=1500, bs=10, pt=30, learn_type ='neurals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activate_brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activate_brain['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = list()\n",
    "for a in activate_brain['predictions']:\n",
    "    if a < 0 :\n",
    "        original.append(0)\n",
    "    elif a > 0:\n",
    "        original.append(a)\n",
    "# cat_original = list()\n",
    "# for a in cat_pred:\n",
    "#     if a < 0 :\n",
    "#         cat_original.append(0)\n",
    "#     elif a > 0:\n",
    "#         cat_original.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel['Item_Store_Returns'] = original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.to_csv('nn7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = pd.read_csv('xgb2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol['Item_Store_Returns'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
